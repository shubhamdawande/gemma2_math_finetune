{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9133365,"sourceType":"datasetVersion","datasetId":5514753}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nimport torch\n\nmodel_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\ntoken = \"\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_id,\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device_map=\"auto\",\n    token=token\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\nfrom pprint import pprint\n\ndef compare(gt, pred):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You will be given a ground truth answer and a model answer. Please output ACCURATE if the model answer matches the ground truth answer or INACCURATE otherwise. Please only return ACCURATE or INACCURATE. It is very important for my job that you do this.\"},\n        {\"role\": \"user\", \"content\": f\"\"\"<GroundTruthAnswer>\n{gt}\n</GroundTruthAnswer>\n\n<ModelAnswer>\n{pred}\n</ModelAnswer>\"\"\"},\n    ]\n    \n#     pprint(messages)\n\n    outputs = pipeline(\n        messages,\n        max_new_tokens=10,\n    )\n    return outputs[0][\"generated_text\"][-1]['content'].lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport json\n\nfpath = \"/kaggle/input/gemma-responses/gemma2_response.jsonl\"\noutpath = \"/kaggle/working/gemma2_response_rated.jsonl\"\n\nwith open(outpath, 'w+') as ff:\n    with open(fpath, 'r') as f:\n        data = list(f)\n        for idx, el in enumerate(data):\n            item = json.loads(el)\n            gt = item['groundtruth'].replace('<end_of_turn>', '')\n            pred = item['prediction'].replace('<end_of_turn><eos>', '')\n            response = compare(gt, pred)\n            print(idx)\n            print(response)\n            ff.write(json.dumps({\"index\": item['index'], \"accurate\": response}) + '\\n')\n#             if idx == 100:\n#                 break","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fpath = \"/kaggle/input/gemma-responses/gemma2_finetuned_response.jsonl\"\noutpath = \"/kaggle/working/gemma2_finetuned_response_rated.jsonl\"\n\nwith open(outpath, 'w+') as ff:\n    with open(fpath, 'r') as f:\n        data = list(f)\n        for idx, el in enumerate(data):\n            item = json.loads(el)\n            gt = item['groundtruth'].replace('<end_of_turn>', '')\n            pred = item['prediction'].replace('<end_of_turn><eos>', '')\n            response = compare(gt, pred)\n            print(idx)\n            print(response)\n            ff.write(json.dumps({\"index\": item['index'], \"accurate\": response}) + '\\n')\n#             if idx == 100:\n#                 break","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]}]}