{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install --upgrade transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import transformers\n","import torch\n","\n","model_id = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n","token = \"\"\n","\n","pipeline = transformers.pipeline(\n","    \"text-generation\",\n","    model=model_id,\n","    model_kwargs={\"torch_dtype\": torch.bfloat16},\n","    device_map=\"auto\",\n","    token=token\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.backends.cuda.enable_mem_efficient_sdp(False)\n","torch.backends.cuda.enable_flash_sdp(False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import json\n","from pprint import pprint\n","\n","def compare(gt, pred):\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"You will be given a ground truth answer and a model answer. Please output ACCURATE if the model answer matches the ground truth answer or INACCURATE otherwise. Please only return ACCURATE or INACCURATE. It is very important for my job that you do this.\"},\n","        {\"role\": \"user\", \"content\": f\"\"\"<GroundTruthAnswer>\n","{gt}\n","</GroundTruthAnswer>\n","\n","<ModelAnswer>\n","{pred}\n","</ModelAnswer>\"\"\"},\n","    ]\n","    \n","#     pprint(messages)\n","\n","    outputs = pipeline(\n","        messages,\n","        max_new_tokens=10,\n","    )\n","    return outputs[0][\"generated_text\"][-1]['content'].lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["import os\n","import json\n","\n","fpath = \"/kaggle/input/gemma-responses/gemma2_response.jsonl\"\n","outpath = \"/kaggle/working/gemma2_response_rated.jsonl\"\n","\n","with open(outpath, 'w+') as ff:\n","    with open(fpath, 'r') as f:\n","        data = list(f)\n","        for idx, el in enumerate(data):\n","            item = json.loads(el)\n","            gt = item['groundtruth'].replace('<end_of_turn>', '')\n","            pred = item['prediction'].replace('<end_of_turn><eos>', '')\n","            response = compare(gt, pred)\n","            print(idx)\n","            print(response)\n","            ff.write(json.dumps({\"index\": item['index'], \"accurate\": response}) + '\\n')\n","#             if idx == 100:\n","#                 break"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["fpath = \"/kaggle/input/gemma-responses/gemma2_finetuned_response.jsonl\"\n","outpath = \"/kaggle/working/gemma2_finetuned_response_rated.jsonl\"\n","\n","with open(outpath, 'w+') as ff:\n","    with open(fpath, 'r') as f:\n","        data = list(f)\n","        for idx, el in enumerate(data):\n","            item = json.loads(el)\n","            gt = item['groundtruth'].replace('<end_of_turn>', '')\n","            pred = item['prediction'].replace('<end_of_turn><eos>', '')\n","            response = compare(gt, pred)\n","            print(idx)\n","            print(response)\n","            ff.write(json.dumps({\"index\": item['index'], \"accurate\": response}) + '\\n')\n","#             if idx == 100:\n","#                 break"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5514753,"sourceId":9133365,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
